# Cross-Attention Fusion配置文件

# 模型配置
fusion_type: 'cross_attention'
text_model: 'roberta-base'
image_model: 'efficientnet_b4'
num_classes: 3
dropout: 0.3
num_heads: 8  # Multi-head attention heads
freeze_backbone: true  # 冻结预训练backbone

# 数据配置
data_dir: 'data'
train_file: 'data_split/train_split.csv'
val_file: 'data_split/val_split.csv'
max_text_length: 128
image_size: 224
augment: false

# 训练配置
batch_size: 32
num_workers: 0
epochs: 15  # 允许训练更多轮，依赖early stopping
backbone_lr: 1.0e-5     # 微调backbone
projection_lr: 1.0e-3   # 训练投影层
classifier_lr: 1.0e-3   # 训练分类头
weight_decay: 0.01
seed: 42

# Early Stopping配置
early_stopping:
  enabled: true
  patience: 3  # 验证集3个epoch不提升则停止
  min_delta: 0.001

# 其他配置
checkpoint_dir: 'checkpoints/cross_attention'
log_dir: 'experiments/logs/cross_attention'
pretrained: true
