# å¤šæ¨¡æ€èåˆæ¨¡å‹è®­ç»ƒè„šæœ¬
# æ”¯æŒ3ç§èåˆæ–¹æ³•ï¼šEarly Fusion, Late Fusion, Cross-Attention Fusion

print("=" * 60)
print("å¯åŠ¨è®­ç»ƒè„šæœ¬...")
print("=" * 60)

import os
import sys

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…å¤šçº¿ç¨‹å†²çª
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'

import yaml
print("æ­£åœ¨å¯¼å…¥PyTorch...")
import torch
torch.set_num_threads(1)  # é™åˆ¶çº¿ç¨‹æ•°
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import argparse
from tqdm import tqdm
import numpy as np

print("æ­£åœ¨å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—...")
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.fusion_models import EarlyFusionModel, LateFusionModel, CrossAttentionFusion
from dataset import MultimodalDataset, TextPreprocessor, get_image_transforms
from utils import set_seed, get_device, AverageMeter
print("å¯¼å…¥å®Œæˆï¼")


class EarlyStopping:
    """Early Stoppingå·¥å…·"""
    
    def __init__(self, patience=5, min_delta=0.001, mode='max'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == 'max':
            if score > self.best_score + self.min_delta:
                self.best_score = score
                self.counter = 0
                return False
            else:
                self.counter += 1
        else:
            if score < self.best_score - self.min_delta:
                self.best_score = score
                self.counter = 0
                return False
            else:
                self.counter += 1
        
        if self.counter >= self.patience:
            self.early_stop = True
            return True
        
        return False



def train_epoch(model, train_loader, criterion, optimizer, device, epoch, writer, config):
    # è®­ç»ƒä¸€ä¸ªepoch
    model.train()
    losses = AverageMeter()
    accuracies = AverageMeter()
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}]')
    
    for batch_idx, batch in enumerate(pbar):
        # ä»å­—å…¸ä¸­æå–æ•°æ®
        text = batch['text']
        images = batch['image']
        labels = batch['label']
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        images = images.to(device)
        labels = labels.to(device)
        text_input = {k: v.to(device) for k, v in text.items()}
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        logits = model(text_input, images, mode='both')
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # è®¡ç®—å‡†ç¡®ç‡
        preds = torch.argmax(logits, dim=1)
        correct = (preds == labels).sum().item()  # æ­£ç¡®æ ·æœ¬æ•°
        batch_acc = correct / images.size(0)  # å½“å‰batchå‡†ç¡®ç‡
        
        # æ›´æ–°ç»Ÿè®¡
        losses.update(loss.item(), images.size(0))
        accuracies.update(batch_acc, images.size(0))  # ä¼ å…¥å‡†ç¡®ç‡æ¯”ä¾‹ï¼Œè¿›è¡ŒåŠ æƒå¹³å‡
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{losses.avg:.4f}',
            'acc': f'{accuracies.avg:.4f}'
        })
        
        # TensorBoardè®°å½•
        global_step = epoch * len(train_loader) + batch_idx
        batch_acc = correct / images.size(0)
        writer.add_scalar('Train/Loss', losses.avg, global_step)
        writer.add_scalar('Train/Acc', batch_acc, global_step)
        
        # é‡Šæ”¾ä¸­é—´å˜é‡å†…å­˜
        del text_input, images, labels, logits, loss, preds
        if batch_idx % 10 == 0:  # æ¯10ä¸ªbatchæ¸…ç†ä¸€æ¬¡
            import gc
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return losses.avg, accuracies.avg


def validate(model, val_loader, criterion, device, mode='both'):
    # éªŒè¯æ¨¡å‹
    model.eval()
    losses = AverageMeter()
    accuracies = AverageMeter()
    
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc='Validating', bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}]'):
            # ä»å­—å…¸ä¸­æå–æ•°æ®
            text = batch['text']
            images = batch['image']
            labels = batch['label']
            
            images = images.to(device)
            labels = labels.to(device)
            text_input = {k: v.to(device) for k, v in text.items()}
            
            # å‰å‘ä¼ æ’­
            logits = model(text_input, images, mode=mode)
            loss = criterion(logits, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            preds = torch.argmax(logits, dim=1)
            correct = (preds == labels).sum().item()  # æ­£ç¡®æ ·æœ¬æ•°
            batch_acc = correct / images.size(0)  # å½“å‰batchå‡†ç¡®ç‡
            
            # æ›´æ–°ç»Ÿè®¡
            losses.update(loss.item(), images.size(0))
            accuracies.update(batch_acc, images.size(0))  # ä¼ å…¥å‡†ç¡®ç‡æ¯”ä¾‹ï¼Œè¿›è¡ŒåŠ æƒå¹³å‡
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    return losses.avg, accuracies.avg, all_preds, all_labels


def main():
    parser = argparse.ArgumentParser(description='Train multimodal fusion model')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„')
    args = parser.parse_args()
    
    print("å¼€å§‹åŠ è½½é…ç½®...")
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config['seed'])
    
    # è®¾ç½®è®¾å¤‡
    device = get_device()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    print("æ­£åœ¨åŠ è½½tokenizer...")
    # åˆ›å»ºtokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained('roberta-base')
    
    print("æ­£åœ¨åˆ›å»ºæ•°æ®é›†...")
    # åˆ›å»ºæ•°æ®é›†
    text_preprocessor = TextPreprocessor(max_length=config['max_text_length'])
    train_transform = get_image_transforms(config['image_size'], augment=config['augment'])
    val_transform = get_image_transforms(config['image_size'], augment=False)
    
    train_dataset = MultimodalDataset(
        csv_file=config['train_file'],
        data_dir=config['data_dir'],
        tokenizer=tokenizer,
        text_transform=text_preprocessor,
        image_transform=train_transform,
        max_text_length=config['max_text_length']
    )
    
    val_dataset = MultimodalDataset(
        csv_file=config['val_file'],
        data_dir=config['data_dir'],
        tokenizer=tokenizer,
        text_transform=text_preprocessor,
        image_transform=val_transform,
        max_text_length=config['max_text_length']
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (CPUè®­ç»ƒæ—¶å…³é—­pin_memoryé¿å…è­¦å‘Š)
    use_pin_memory = device.type == 'cuda'
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=use_pin_memory
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=use_pin_memory
    )
    
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    fusion_type = config['fusion_type']
    print(f"èåˆæ–¹æ³•: {fusion_type}")
    
    if fusion_type == 'early':
        model = EarlyFusionModel(
            text_model=config['text_model'],
            image_model=config['image_model'],
            num_classes=config['num_classes'],
            dropout=config['dropout'],
            pretrained=config.get('pretrained', True),
            freeze_backbone=config.get('freeze_backbone', True)
        )
    elif fusion_type == 'late':
        model = LateFusionModel(
            text_model=config['text_model'],
            image_model=config['image_model'],
            num_classes=config['num_classes'],
            dropout=config.get('dropout', 0.1),
            pretrained=config.get('pretrained', True),
            freeze_backbone=config.get('freeze_backbone', True)
        )
    elif fusion_type == 'cross_attention':
        model = CrossAttentionFusion(
            text_model=config['text_model'],
            image_model=config['image_model'],
            num_classes=config['num_classes'],
            dropout=config['dropout'],
            pretrained=config.get('pretrained', True),
            freeze_backbone=config.get('freeze_backbone', True),
            num_heads=config.get('num_heads', 8)
        )
    else:
        raise ValueError(f"Unknown fusion type: {fusion_type}")
    
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
    print(f"å†»ç»“å‚æ•°é‡: {frozen_params:,} ({100*frozen_params/total_params:.1f}%)")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼ˆåˆ†å±‚å­¦ä¹ ç‡ï¼‰
    criterion = nn.CrossEntropyLoss()
    
    # å‚æ•°åˆ†ç»„ï¼šbackboneç”¨å°å­¦ä¹ ç‡å¾®è°ƒï¼Œprojectionå’Œclassifierç”¨å¤§å­¦ä¹ ç‡
    backbone_params = []
    projection_params = []
    classifier_params = []
    
    # æ”¶é›†å„éƒ¨åˆ†å‚æ•°
    if hasattr(model, 'text_encoder'):
        backbone_params.extend(model.text_encoder.encoder.parameters())
        projection_params.extend(model.text_encoder.projection.parameters())
        if hasattr(model.text_encoder, 'classifier'):
            classifier_params.extend(model.text_encoder.classifier.parameters())
    
    if hasattr(model, 'image_encoder'):
        backbone_params.extend(model.image_encoder.encoder.parameters())
        projection_params.extend(model.image_encoder.projection.parameters())
        if hasattr(model.image_encoder, 'classifier'):
            classifier_params.extend(model.image_encoder.classifier.parameters())
    
    # æ”¶é›†èåˆå±‚å’Œå…¶ä»–å‚æ•°
    fusion_params = []
    for name, param in model.named_parameters():
        if 'encoder' not in name and 'projection' not in name and 'text_encoder.classifier' not in name and 'image_encoder.classifier' not in name:
            fusion_params.append(param)
    
    # åˆ›å»ºå‚æ•°ç»„
    param_groups = [
        {'params': backbone_params, 'lr': config['backbone_lr']},
        {'params': projection_params, 'lr': config['projection_lr']},
        {'params': classifier_params + fusion_params, 'lr': config['classifier_lr']}
    ]
    
    optimizer = optim.AdamW(
        param_groups,
        weight_decay=config['weight_decay']
    )
    
    print(f"ğŸ“Š å­¦ä¹ ç‡é…ç½®: backbone={config['backbone_lr']}, projection={config['projection_lr']}, classifier={config['classifier_lr']}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config['epochs']
    )
    
    # TensorBoard
    os.makedirs(config['log_dir'], exist_ok=True)
    writer = SummaryWriter(config['log_dir'])
    
    # åˆ›å»ºcheckpointç›®å½•
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    
    # Early Stopping
    early_stopping = None
    if config.get('early_stopping', {}).get('enabled', False):
        early_stopping = EarlyStopping(
            patience=config['early_stopping']['patience'],
            min_delta=config['early_stopping']['min_delta'],
            mode='max'
        )
        print(f"ğŸ“‰ Early stopping enabled: patience={early_stopping.patience}")
    
    # Resume from checkpoint
    start_epoch = 0
    best_val_acc = 0.0
    if args.resume:
        print(f"æ­£åœ¨ä»checkpointæ¢å¤: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_acc = checkpoint.get('val_acc', 0.0)
        print(f"âœ… ä»epoch {start_epoch}æ¢å¤è®­ç»ƒï¼Œä¹‹å‰æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}")
    
    # è®­ç»ƒå¾ªç¯
    
    for epoch in range(start_epoch, config['epochs']):
        print(f"\nEpoch {epoch+1}/{config['epochs']}")
        print("-" * 50)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, writer, config
        )
        
        print(f"è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}")
        
        # éªŒè¯
        val_loss, val_acc, all_preds, all_labels = validate(model, val_loader, criterion, device, mode='both')
        
        # è®¡ç®—å®é™…å‡†ç¡®ç‡ï¼ˆç”¨äºéªŒè¯ï¼‰
        import numpy as np
        actual_acc = np.mean(np.array(all_preds) == np.array(all_labels))
        
        # æ‰“å°é¢„æµ‹åˆ†å¸ƒï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
        from collections import Counter
        pred_dist = Counter(all_preds)
        label_dist = Counter(all_labels)
        print(f"éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.6f} (å®é™…: {actual_acc:.6f})")
        print(f"é¢„æµ‹åˆ†å¸ƒ: {dict(pred_dist)}, çœŸå®åˆ†å¸ƒ: {dict(label_dist)}")
        
        # åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # TensorBoardè®°å½•
        writer.add_scalar('Epoch/Train_Loss', train_loss, epoch)
        writer.add_scalar('Epoch/Train_Acc', train_acc, epoch)
        writer.add_scalar('Epoch/Val_Loss', val_loss, epoch)
        writer.add_scalar('Epoch/Val_Acc', val_acc, epoch)
        writer.add_scalar('Epoch/LR', optimizer.param_groups[0]['lr'], epoch)
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            checkpoint_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'config': config
            }, checkpoint_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.4f})")
        
        # å®šæœŸä¿å­˜checkpoint
        if (epoch + 1) % 5 == 0:
            checkpoint_path = os.path.join(config['checkpoint_dir'], f'checkpoint_epoch_{epoch+1}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'config': config
            }, checkpoint_path)
        
        # Early Stoppingæ£€æŸ¥
        if early_stopping:
            if early_stopping(val_acc):
                print(f"\nğŸ›‘ Early stopping triggered at epoch {epoch+1}")
                print(f"   Best val acc: {early_stopping.best_score:.4f}")
                break
    
    writer.close()
    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")


if __name__ == '__main__':
    main()
